## Chat com Assistente Virtual ü§ñüí¨

Este projeto √© uma aplica√ß√£o que simula um chat interativo com um assistente virtual. Ele utiliza armazenamento local para gerenciar o hist√≥rico de conversas e implementa o padr√£o **Backend for Frontend (BFF)** com Next.js, permitindo a integra√ß√£o com diferentes modelos de IA ou respostas fixadas/aleat√≥rias.

---

## üöÄ Tecnologias Utilizadas

### Linguagens e Frameworks

- **[Next.js 15](https://nextjs.org/)**: Framework React moderno para aplica√ß√µes web escal√°veis e perform√°ticas.
- **[TypeScript](https://www.typescriptlang.org/)**: Adiciona tipagem est√°tica ao JavaScript para maior confiabilidade no c√≥digo.
- **[TailwindCSS](https://tailwindcss.com/)**: Framework CSS utilit√°rio para estiliza√ß√£o r√°pida e responsiva.
- **[shadcn UI](https://shadcn.dev/)**: Biblioteca de componentes de interface elegantes e acess√≠veis.
- **[Framer Motion](https://www.framer.com/motion/)**: Biblioteca para criar anima√ß√µes fluidas e intera√ß√µes din√¢micas.

---

## üìù Funcionalidades

1. **Chat com Assistente Virtual**:

   - Envio de mensagens pelo usu√°rio com resposta autom√°tica do assistente.
   - Integra√ß√£o com modelos de IA ou uso de respostas fixadas/aleat√≥rias.
   - Respostas podem ser geradas via array predefinido ou por modelos de IA.

2. **Sele√ß√£o de respostas**:

   - **Modo IA ativado**: O usu√°rio pode escolher entre os modelos dispon√≠veis.
     - Modelos suportados:
       - **Groq API**:
         - llama-3.3-70b-versatile
         - llama-3.1-8b-instant
         - llama3-70b-8192
         - llama3-8b-8192
         - gemma2-9b-it
       - **OpenAI API**:
         - gpt-3.5-turbo-instruct
   - **Modo IA desativado**: O assistente responde utilizando frases predefinidas em um array local.

3. **Hist√≥rico de Conversas**:

   - Armazenamento local (localStorage) das conversas para persist√™ncia ap√≥s o refresh da p√°gina.
   - Possibilidade de renomear os chats para melhor organiza√ß√£o.

4. **Gest√£o de Conversas**:

   - Cria√ß√£o de novos chats.
   - Exclus√£o de chats existentes.
   - Renomea√ß√£o de chats diretamente pela interface.

5. **Integra√ß√£o com Backend for Frontend (BFF)**:
   - Uso do App Router do Next.js para gerenciar mensagens entre cliente e servidor.
   - Endpoints para processar mensagens e listar modelos dispon√≠veis.

---

## üõ†Ô∏è Como Rodar o Projeto Localmente

1. Clone o reposit√≥rio:

   ```bash
      git clone https://github.com/viniciusnreno/chat-interativo-tech4humans.git
   ```

2. Navegue at√© o diret√≥rio do projeto:

   ```bash
   cd chat-interativo-tech4humans
   ```

3. Instale as depend√™ncias:

   ```bash
   npm install
   ```

4. Configure as vari√°veis de ambiente:

   - Crie um arquivo `.env` na raiz do projeto e adicione as chaves de API necess√°rias:

   ```bash
   OPENAI_API_KEY=sua_openai_api_key
   GROQ_API_KEY=sua_groq_api_key
   ```

6. Execute o servidor de desenvolvimento:

   ```bash
   npm run dev
   ```

7. Acesse a aplica√ß√£o em: http://localhost:3000

---

## üåê Link Hospedado

Acesse a vers√£o online do projeto: [https://chat-interativo-tech4humans.vercel.app/](https://chat-interativo-tech4humans.vercel.app/).

## üåü Estrutura do Projeto

- app/api/chat/[model]/route.ts: Endpoint respons√°vel por processar mensagens via modelo de IA.
- app/api/chat/presetList/route.ts: Endpoint que retorna respostas gen√©ricas baseadas no conte√∫do da mensagem recebida.
- app/page.tsx: P√°gina principal do chat.
- components/: Cont√©m os componentes reutiliz√°veis como formul√°rio de envio, conte√∫do do chat, e itens de interface.
- contexts/chat-context.tsx: Gerenciamento de estado e l√≥gica de conversas.
- hooks/useMessages.ts: Hook respons√°vel por gerenciar o estado das mensagens, realizar chamadas aos endpoints e atualizar o hist√≥rico do chat.
- utils/chatService.ts: Gerenciamento do hist√≥rico e manipula√ß√£o de chats no localStorage.
- utils/userService.ts: Fun√ß√µes para salvar e recuperar o nome do usu√°rio.
- utils/handleRequest.tsx: Utilit√°rio para chamadas √†s APIs.

---

## üìú Padr√£o Backend for Frontend (BFF)

O padr√£o BFF foi implementado atrav√©s do App Router do Next.js:

- Endpoint /api/chat/[model]: Gerencia a l√≥gica de mensagens e respostas do assistente virtual com suporte a m√∫ltiplos modelos de IA.
- Endpoint /api/chat/presetList: Retorna respostas predefinidas ou gen√©ricas baseadas no contexto da mensagem recebida.

---

## ü§ñ Ollama - Modelos de IA Locais

**Ollama** √© uma ferramenta que permite executar modelos de IA localmente em sua m√°quina, garantindo maior privacidade e controle sobre os dados. Este projeto utiliza o modelo **deepseek-r1:8b**, que roda localmente e est√° acess√≠vel atrav√©s da porta `http://127.0.0.1:11434/api/chat`.

### Como instalar e configurar o Ollama:

1. Acesse o site oficial do Ollama: [https://ollama.ai](https://ollama.ai).
2. Fa√ßa o download e instale a ferramenta de acordo com o sistema operacional da sua m√°quina.
3. Ap√≥s a instala√ß√£o, baixe o modelo **deepseek-r1:8b** com o seguinte comando:
   
   ```bash
   ollama run deepseek-r1:8b
   ```
   
4. Certifique-se de que o servi√ßo do Ollama est√° rodando corretamente. Para verificar o status, utilize o comando:

   ```bash
   systemctl status ollama.service
   ```
   
   Se o servi√ßo n√£o estiver ativo, inicie-o com o comando:
   
   ```bash
   ollama serve
   ```

üí° Desenvolvido por [Vinicius Ren√≥](https://viniciusreno.vercel.app/). üöÄ
