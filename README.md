## Chat com Assistente Virtual ü§ñüí¨

Este projeto √© uma aplica√ß√£o frontend que simula um chat interativo com um assistente virtual. Ele utiliza armazenamento local para gerenciar o hist√≥rico de conversas e implementa o padr√£o **Backend for Frontend (BFF)** com Next.js, permitindo a integra√ß√£o com diferentes modelos de IA ou respostas fixadas/aleat√≥rias.

---

## üöÄ Tecnologias Utilizadas

### Linguagens e Frameworks

- **[Next.js 15](https://nextjs.org/)**: Framework React moderno para aplica√ß√µes web escal√°veis e perform√°ticas.
- **[TypeScript](https://www.typescriptlang.org/)**: Adiciona tipagem est√°tica ao JavaScript para maior confiabilidade no c√≥digo.
- **[TailwindCSS](https://tailwindcss.com/)**: Framework CSS utilit√°rio para estiliza√ß√£o r√°pida e responsiva.
- **[shadcn UI](https://shadcn.dev/)**: Biblioteca de componentes de interface elegantes e acess√≠veis.
- **[Sonner](https://sonner.dev/)**: Biblioteca para exibi√ß√£o de notifica√ß√µes din√¢micas e estilizadas.

---

## üìù Funcionalidades

1. **Chat com Assistente Virtual**:
   - Envio de mensagens pelo usu√°rio com resposta autom√°tica do assistente.
   - Integra√ß√£o com modelos de IA ou uso de respostas fixadas/aleat√≥rias.
   - Respostas podem ser geradas via array predefinido ou por modelos de IA.

2. **Switch de IA**:
   - **Modo IA ativado**: O usu√°rio pode escolher entre os modelos dispon√≠veis.
     - Modelos suportados:
       - **Groq API**:
         - llama-3.3-70b-versatile
         - llama-3.1-8b-instant
         - llama3-70b-8192
         - llama3-8b-8192
         - gemma2-9b-it
       - **OpenAI API**:
         - gpt-3.5-turbo-instruct
       - **Local (Ollama)**:
         - deepseek-r1:8b (local)
   - **Modo IA desativado**: O assistente responde utilizando frases predefinidas em um array local.

3. **Hist√≥rico de Conversas**:
   - Armazenamento local (localStorage) das conversas para persist√™ncia ap√≥s o refresh da p√°gina.
   - Possibilidade de renomear os chats para melhor organiza√ß√£o.

4. **Gest√£o de Conversas**:
   - Cria√ß√£o de novos chats.
   - Exclus√£o de chats existentes.
   - Renomea√ß√£o de chats diretamente pela interface.

5. **Integra√ß√£o com Backend for Frontend (BFF)**:
   - Uso do App Router do Next.js para gerenciar mensagens entre cliente e servidor.
   - Endpoints para processar mensagens e listar modelos dispon√≠veis.

---

## üõ†Ô∏è Como Rodar o Projeto Localmente

1. Clone o reposit√≥rio:

   ```bash
      git clone https://github.com/viniciusnreno/chat-interativo-tech4humans.git
   ```

2. Navegue at√© o diret√≥rio do projeto:

   ```bash
   cd chat-interativo-tech4humans
   ```

3. Instale as depend√™ncias:

   ```bash
   npm install
   ```

4. Configure as vari√°veis de ambiente:
   - Crie um arquivo `.env` na raiz do projeto e adicione as chaves de API necess√°rias:
   ```bash
   OPENAI_API_KEY=sua_openai_api_key
   GROQ_API_KEY=sua_groq_api_key
   ```

5. Instale o **Ollama** (opcional para o modelo local):
- Siga as instru√ß√µes no [site oficial do Ollama](https://ollama.ai) para configurar o modelo `deepseek-r1:8b`.

6. Execute o servidor de desenvolvimento:
   ```bash
   npm run dev
   ```

7. Acesse a aplica√ß√£o em: http://localhost:3000

---

## üåê Link Hospedado

Acesse a vers√£o online do projeto: [https://chat-interativo-tech4humans.vercel.app/](https://chat-interativo-tech4humans.vercel.app/).

 

## üåü Estrutura do Projeto

- app/api/chat/[model]/route.ts: Endpoint respons√°vel por processar mensagens via modelo de IA.
- app/api/chat/presetList/route.ts: Endpoint para listar modelos dispon√≠veis.
- app/page.tsx: P√°gina principal do chat.
- components/: Cont√©m os componentes reutiliz√°veis como formul√°rio de envio, conte√∫do do chat, e itens de interface.
- contexts/chat-context.tsx: Gerenciamento de estado e l√≥gica de conversas.
- utils/chatService.ts: Gerenciamento do hist√≥rico e manipula√ß√£o de chats no localStorage.
- utils/userService.ts: Fun√ß√µes para salvar e recuperar o nome do usu√°rio.
- utils/handleRequest.tsx: Utilit√°rio para chamadas √†s APIs.

---

## üìú Padr√£o Backend for Frontend (BFF)

O padr√£o BFF foi implementado atrav√©s do App Router do Next.js:

- Endpoint /api/chat/[model]: Gerencia a l√≥gica de mensagens e respostas do assistente virtual com suporte a m√∫ltiplos modelos de IA.
- Endpoint /api/chat/presetList: Retorna a lista de modelos dispon√≠veis para sele√ß√£o no front-end.

---

üí° Desenvolvido por [Vinicius Ren√≥](https://viniciusreno.vercel.app/). üöÄ
